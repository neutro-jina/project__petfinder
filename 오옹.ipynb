{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "mount_file_id": "162XMBYtJUgaCt-3Z9xuxsIJj7m01KtYn",
      "authorship_tag": "ABX9TyOd2ECD9zRjNXqCejK32z9u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neutro-jina/project__petfinder/blob/main/%EC%98%A4%EC%98%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "m3JnaULNtT_n"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/swin_test/timm-pytorch-image-models/pytorch-image-models-master')\n",
        "sys.path.append('/content/swin_test/earlystoppingpytorch/early-stopping-pytorch')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "#!pip install path\n",
        "#!pip install timm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import path\n",
        "import random\n",
        "import cv2\n",
        "import timm\n",
        "import gc\n",
        "import albumentations\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "9IYmYguBtYTe"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "W_OELDpitcnZ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "#import EarlyStopping\n",
        "#from pytorchtools import EarlyStopping\n",
        "\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "qtVqzHMytt94"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SKlearn Libraries\n",
        "from sklearn import datasets\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Deciding the device used for calculation. CUDA = GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "_E9CrPDGtziI"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from zipfile import ZipFile\n",
        "\n",
        "#with ZipFile(\"/content/drive/MyDrive/Colab Notebooks/petfinder-pawpularity-score.zip\", 'r') as zip:\n",
        "#    zip.extractall('swin_test')\n",
        "#    print('file is unzipped in swin_test folder')"
      ],
      "metadata": {
        "id": "yq5Pw2g1u2vQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv(\"/content/swin_test/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/swin_test/test.csv\")"
      ],
      "metadata": {
        "id": "h7T_suHrt1Le"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv(\"/content/swin_test/train.csv\")\n",
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "gOFGd-xvt3M9",
        "outputId": "4bc9e8dd-46ac-42e3-8c80-beb3139fda34"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    Id  Subject Focus  Eyes  Face  Near  \\\n",
              "0     0007de18844b0dbbb5e1f607da0606e0              0     1     1     1   \n",
              "1     0009c66b9439883ba2750fb825e1d7db              0     1     1     0   \n",
              "2     0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1   \n",
              "3     0018df346ac9c1d8413cfcc888ca8246              0     1     1     1   \n",
              "4     001dc955e10590d3ca4673f034feeef2              0     0     0     1   \n",
              "...                                ...            ...   ...   ...   ...   \n",
              "9907  ffbfa0383c34dc513c95560d6e1fdb57              0     0     0     1   \n",
              "9908  ffcc8532d76436fc79e50eb2e5238e45              0     1     1     1   \n",
              "9909  ffdf2e8673a1da6fb80342fa3b119a20              0     1     1     1   \n",
              "9910  fff19e2ce11718548fa1c5d039a5192a              0     1     1     1   \n",
              "9911  fff8e47c766799c9e12f3cb3d66ad228              0     1     1     1   \n",
              "\n",
              "      Action  Accessory  Group  Collage  Human  Occlusion  Info  Blur  \\\n",
              "0          0          0      1        0      0          0     0     0   \n",
              "1          0          0      0        0      0          0     0     0   \n",
              "2          0          0      0        0      1          1     0     0   \n",
              "3          0          0      0        0      0          0     0     0   \n",
              "4          0          0      1        0      0          0     0     0   \n",
              "...      ...        ...    ...      ...    ...        ...   ...   ...   \n",
              "9907       0          0      0        0      0          0     0     1   \n",
              "9908       0          0      0        0      0          0     0     0   \n",
              "9909       0          0      0        0      1          1     0     0   \n",
              "9910       0          0      0        0      1          0     0     0   \n",
              "9911       0          0      0        0      0          0     0     0   \n",
              "\n",
              "      Pawpularity  \n",
              "0              63  \n",
              "1              42  \n",
              "2              28  \n",
              "3              15  \n",
              "4              72  \n",
              "...           ...  \n",
              "9907           15  \n",
              "9908           70  \n",
              "9909           20  \n",
              "9910           20  \n",
              "9911           30  \n",
              "\n",
              "[9912 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78985452-6c35-4d17-8f04-db18a1d9ebda\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Subject Focus</th>\n",
              "      <th>Eyes</th>\n",
              "      <th>Face</th>\n",
              "      <th>Near</th>\n",
              "      <th>Action</th>\n",
              "      <th>Accessory</th>\n",
              "      <th>Group</th>\n",
              "      <th>Collage</th>\n",
              "      <th>Human</th>\n",
              "      <th>Occlusion</th>\n",
              "      <th>Info</th>\n",
              "      <th>Blur</th>\n",
              "      <th>Pawpularity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9907</th>\n",
              "      <td>ffbfa0383c34dc513c95560d6e1fdb57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9908</th>\n",
              "      <td>ffcc8532d76436fc79e50eb2e5238e45</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9909</th>\n",
              "      <td>ffdf2e8673a1da6fb80342fa3b119a20</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9910</th>\n",
              "      <td>fff19e2ce11718548fa1c5d039a5192a</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9911</th>\n",
              "      <td>fff8e47c766799c9e12f3cb3d66ad228</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9912 rows Ã— 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78985452-6c35-4d17-8f04-db18a1d9ebda')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-78985452-6c35-4d17-8f04-db18a1d9ebda button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-78985452-6c35-4d17-8f04-db18a1d9ebda');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = ['Pawpularity']\n",
        "not_features = ['Id', 'kfold', 'image_path', 'Pawpularity']\n",
        "cols = list(data_df.columns)\n",
        "features = [feat for feat in cols if feat not in not_features]\n",
        "print(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5Vq8AJZyZ-1",
        "outputId": "85090c1c-8bb3-425c-edfb-6019092b8dda"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'folder_dir': '/content/swin_test/',\n",
        "    'model':'swin_large_patch4_window12_384',\n",
        "    'image_dir':  '/content/swin_test/train/',\n",
        "    'test_img_dir': '/content/swin_test/test/',\n",
        "    'features': features,\n",
        "    'img_size' : 384,\n",
        "    'dropout':0.4,\n",
        "    'num_workers':2,\n",
        "    'fold' : 10,\n",
        "    'batch_size' : 4,\n",
        "    'lr' : 1e-5,\n",
        "    'scheduler_name': 'CosineAnnealingWarmRestarts',\n",
        "    'T_0':50,\n",
        "    'min_lr':1e-7,\n",
        "    'pretrained':True,\n",
        "    'weight_decay':1e-6\n",
        "}\n",
        "\n",
        "# Setting manual seed to everything.\n",
        "# So that we will get the same results everything we run the notebook.\n",
        "SEED = 42\n",
        "\n",
        "def seed_everything(seed=SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "seed_everything()"
      ],
      "metadata": {
        "id": "WtQu2Ul2ycOU"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PawDataSet():\n",
        "    def __init__(self,dataset, params, features, transform = None,):\n",
        "        self.dataset = dataset\n",
        "        self.image_path = dataset['Id'].apply(lambda x: os.path.join(params['image_dir'],f'{x}.jpg'))\n",
        "        self.target_label = dataset['Pawpularity']\n",
        "        self.features = dataset[features].values\n",
        "        self.class_label = self.target_label/100\n",
        "        self.transform = transform\n",
        "        self.params = params\n",
        "    \n",
        "    # Returen the len of data.\n",
        "    def __len__(self):\n",
        "        return len(self.image_path)\n",
        "    \n",
        "    # Load images and target score according to index number (idx)\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.image_path[idx]\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image=image)['image']\n",
        "        \n",
        "        image = np.transpose(image,(2, 0, 1)).astype(np.float32)\n",
        "        image = torch.tensor(image)\n",
        "        features = self.features[idx, :]\n",
        "        targets = torch.tensor(self.class_label[idx]).float()\n",
        "        \n",
        "        return image, features, targets"
      ],
      "metadata": {
        "id": "RzUU5SGjyeJg"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmentation function for the training data.\n",
        "def Transform_train(DIM = params['img_size']):\n",
        "    return albumentations.Compose(\n",
        "        [\n",
        "            albumentations.Resize(DIM,DIM),\n",
        "            albumentations.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225],\n",
        "            ),\n",
        "            albumentations.HorizontalFlip(p=0.5),\n",
        "            albumentations.VerticalFlip(p=0.5),\n",
        "            albumentations.Rotate(limit=45, p=0.4),\n",
        "            albumentations.ShiftScaleRotate(\n",
        "                shift_limit = 0.1, scale_limit=0.1, rotate_limit=45, p=0.5\n",
        "            ),\n",
        "            albumentations.HueSaturationValue(\n",
        "                hue_shift_limit=0.2, sat_shift_limit=0.2,\n",
        "                val_shift_limit=0.2, p=0.5\n",
        "            ),\n",
        "            albumentations.RandomBrightnessContrast(\n",
        "                brightness_limit=(-0.1, 0.1),\n",
        "                contrast_limit=(-0.1, 0.1), p=0.5\n",
        "            )\n",
        "        ],\n",
        "        p=1.0\n",
        "    )"
      ],
      "metadata": {
        "id": "czf34Iz1ygLW"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmentation function for the validation data.\n",
        "def Transform_val(DIM = params['img_size']):\n",
        "    return albumentations.Compose(\n",
        "        [\n",
        "            albumentations.Resize(DIM, DIM),\n",
        "            albumentations.Normalize(\n",
        "                mean = [0.485, 0.456, 0.406],\n",
        "                std = [0.229, 0.224, 0.225],\n",
        "                max_pixel_value=255.0,\n",
        "                p = 1.0\n",
        "            ),\n",
        "        ],\n",
        "        p=1.0\n",
        "    )"
      ],
      "metadata": {
        "id": "N6krBGGIyiON"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Sturges' rule to determine the best number of bins for our data.\n",
        "num_bins = int(np.floor(1+np.log2(len(data_df))))"
      ],
      "metadata": {
        "id": "KwRgFAb5ykNX"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['bins'] = pd.cut(data_df['Pawpularity'], bins=num_bins, labels=False)\n",
        "data_df['fold'] = -1\n",
        "\n",
        "# Function to create Folds.\n",
        "def create_folds(data, num_splits):\n",
        "    strat_kfold = StratifiedKFold(n_splits=num_splits, random_state=SEED, shuffle=True)\n",
        "    for i, (_, idx) in enumerate(strat_kfold.split(data_df.index, data_df['bins'])):\n",
        "        data_df.iloc[idx, -1] = i\n",
        "    \n",
        "    data_df['fold'] = data_df['fold'].astype('int')\n",
        "    data_df.fold.value_counts().plot.bar(xlabel=\"Fold\", ylabel=\"Number of data\")"
      ],
      "metadata": {
        "id": "9vFTiXeCyl9F"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Folds\n",
        "df_5 = create_folds(data_df, num_splits=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "l8xeNvbpynsb",
        "outputId": "7da7f3f6-b2ee-4d9f-d7dc-dfb84d5fcd02"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 Folds\n",
        "df_10 = create_folds(data_df, num_splits=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "e6pbHf53ypbj",
        "outputId": "d496b90d-abeb-4508-90a5-7626afe3d7fc"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def usr_rmse_score(output, target):\n",
        "    y_pred = torch.sigmoid(output).cpu()\n",
        "    y_pred = y_pred.detach().numpy()*100\n",
        "    target = target.cpu()*100\n",
        "    \n",
        "    return mean_squared_error(target, y_pred, squared=False)"
      ],
      "metadata": {
        "id": "bTBrZ6xqyrE2"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricMonitor:\n",
        "    def __init__(self, float_precision=3):\n",
        "        self.float_precision = float_precision\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        self.metrics = defaultdict(lambda: {'val':0, 'count':0, 'avg':0})\n",
        "    \n",
        "    def update(self, metric_name, val):\n",
        "        metric = self.metrics[metric_name]\n",
        "        \n",
        "        metric['val'] += val\n",
        "        metric['count'] += 1\n",
        "        metric['avg'] = metric['val'] / metric['count']\n",
        "        \n",
        "    def __str__(self):\n",
        "        return \"|\".join(\n",
        "            [\n",
        "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
        "                    metric_name=metric_name, avg=metric['avg'],\n",
        "                    float_precision=self.float_precision\n",
        "                )\n",
        "                for (metric_name, metric) in self.metrics.items()\n",
        "            ]\n",
        "        )\n",
        "    "
      ],
      "metadata": {
        "id": "-_OrVKpkyswU"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scheduler(optimizer, scheduler_params=params):\n",
        "    if scheduler_params['scheduler_name'] == 'CosineAnnealingWarmRestarts':\n",
        "        scheduler = CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0 = scheduler_params['T_0'],\n",
        "            eta_min = scheduler_params['min_lr'],\n",
        "            last_epoch = -1\n",
        "        )\n",
        "    return scheduler"
      ],
      "metadata": {
        "id": "xR4dtZmMyuQf"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and print out the architecture of the pretrained model.\n",
        "# We will change only the last layer of the model(head) in the next column.\n",
        "SWIN_model = timm.create_model(model_name = params['model'])\n",
        "print(SWIN_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jXY_ByxywDh",
        "outputId": "a4654cb4-6715-4951-e9ba-334f93b7b23d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SwinTransformer(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
            "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (layers): Sequential(\n",
            "    (0): BasicLayer(\n",
            "      dim=192, input_resolution=(96, 96), depth=2\n",
            "      (blocks): ModuleList(\n",
            "        (0): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        input_resolution=(96, 96), dim=192\n",
            "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
            "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicLayer(\n",
            "      dim=384, input_resolution=(48, 48), depth=2\n",
            "      (blocks): ModuleList(\n",
            "        (0): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        input_resolution=(48, 48), dim=384\n",
            "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (2): BasicLayer(\n",
            "      dim=768, input_resolution=(24, 24), depth=18\n",
            "      (blocks): ModuleList(\n",
            "        (0): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (12): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (13): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (14): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (15): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (16): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (17): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        input_resolution=(24, 24), dim=768\n",
            "        (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
            "        (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (3): BasicLayer(\n",
            "      dim=1536, input_resolution=(12, 12), depth=2\n",
            "      (blocks): ModuleList(\n",
            "        (0): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            (softmax): Softmax(dim=-1)\n",
            "          )\n",
            "          (drop_path): DropPath()\n",
            "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "            (act): GELU()\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
            "  (head): Linear(in_features=1536, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PetNet(nn.Module):\n",
        "    def __init__(self, model_name=params['model'], pretrained=params['pretrained'], features=len(params['features']) ):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(model_name=model_name, pretrained=pretrained, in_chans=3)\n",
        "        # Replace the final head layers in model with our own Linear layer\n",
        "        num_features = self.model.head.in_features\n",
        "        self.model.head = nn.Linear(num_features, 128)\n",
        "        self.fully_connect = nn.Sequential(nn.Linear(128 + features, 64),\n",
        "                                           nn.ReLU(),\n",
        "                                           nn.Linear(64, 1)\n",
        "                                          )\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "    \n",
        "    def forward(self, image, features):\n",
        "        x = self.model(image)\n",
        "        # Using dropout functions to randomly shutdown some of the nodes in hidden layers to prevent overfitting.\n",
        "        x = self.dropout(x)\n",
        "        # Concatenate the metadata into the results.\n",
        "        x = torch.cat([x, features], dim=1)\n",
        "        output = self.fully_connect(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "16miaXG4yyE-"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(train_loader, model, criterion, optimizer ,epoch, params, scheduler=None):\n",
        "    metric_monitor = MetricMonitor()\n",
        "    # Set the model into train model. There are train mode and eval mode.\n",
        "    model.train()\n",
        "    \n",
        "    # Load the data using tqdm to visualize the training process.\n",
        "    stream = tqdm(train_loader)\n",
        "    \n",
        "    for i, (images,features, target) in enumerate(stream):\n",
        "        images = images.to(device)\n",
        "        target = target.to(device).view(-1, 1)\n",
        "        features = features.to(device)\n",
        "        \n",
        "        # Generate predictions by passing images through the model.\n",
        "        preds = model(images, features)\n",
        "        \n",
        "        # Calculate the difference between prediction value and target value ('Pawpularity' label). \n",
        "        loss = criterion(preds, target)\n",
        "        \n",
        "        # Generate Root Mean Square Error score\n",
        "        rmse_score = usr_rmse_score(preds, target)\n",
        "        metric_monitor.update('Loss', loss.item())\n",
        "        metric_monitor.update('RMSE', rmse_score)\n",
        "        \n",
        "        # Generate loss gradient and optimize the weight of model using optimizer.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Use scheduler to change the learning rate.\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "            \n",
        "        # Reset the gradient after each loop. To avoid it from adding up.\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Set description to the progress bar when we run this training function\n",
        "        stream.set_description(f\"Epoch: {epoch:02}. Train. {metric_monitor}\")"
      ],
      "metadata": {
        "id": "Lq5_8gmuy2Ns"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_fn(val_loader, model, criterion, epoch, params):\n",
        "    metric_monitor = MetricMonitor()\n",
        "    \n",
        "    # Set the model into evaluation mode. This will turn off the Dropout layers or BatchNorm layers in the model.\n",
        "    model.eval()\n",
        "    stream = tqdm(val_loader)\n",
        "    valid_targets = []\n",
        "    predictions = []\n",
        "    \n",
        "    # Turn off the gradient tracking for faster processing.\n",
        "    with torch.no_grad():\n",
        "        for i, (images,features, target) in enumerate(stream, start=1):\n",
        "            images = images.to(device)\n",
        "            target = target.float().view(-1, 1)\n",
        "            target = target.to(device)\n",
        "            features = features.to(device)\n",
        "           \n",
        "            preds = model(images, features)\n",
        "            loss = criterion(preds, target)\n",
        "           \n",
        "            rmse_score = usr_rmse_score(preds, target)\n",
        "            metric_monitor.update('Loss', loss.item())\n",
        "            metric_monitor.update('RMSE', rmse_score)\n",
        "            stream.set_description(f'Epoch: {epoch:02}. Valid. {metric_monitor}')\n",
        "            \n",
        "            targets = (target.detach().cpu().numpy()*100).tolist()\n",
        "            outputs = (torch.sigmoid(preds).detach().cpu().numpy()*100).tolist()\n",
        "            \n",
        "            valid_targets.extend(targets)\n",
        "            predictions.extend(outputs)\n",
        "            \n",
        "    return valid_targets, predictions"
      ],
      "metadata": {
        "id": "XOJ9gbaXy4KV"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_models_of_each_fold = []\n",
        "rmse_tracker = []"
      ],
      "metadata": {
        "id": "XUE5_5U3y6hK"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "qULP9kJm9Cv-"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5# Set the range from 0 to 10 to run the whole Folds.\n",
        "for fold in range(1):\n",
        "    # Split the data into training data and validation data for cross validation\n",
        "    # The data that have same label as the fold will be used as Validation data, the rest as Training data.\n",
        "    train = data_df[data_df['fold']!=fold].reset_index(drop=True)\n",
        "    val = data_df[data_df['fold']==fold].reset_index(drop=True)\n",
        "    \n",
        "    # Making training and validating dataset.\n",
        "    train_dataset = PawDataSet(\n",
        "        dataset = train,\n",
        "        params = params,\n",
        "        features = params['features'],\n",
        "        transform = Transform_train()\n",
        "    )\n",
        "    val_dataset = PawDataSet(\n",
        "        dataset = val,\n",
        "        params = params,\n",
        "        features = params['features'],\n",
        "        transform = Transform_val()\n",
        "    )\n",
        "    \n",
        "    # Making data loader using PyTorch DataLoader function. This allow us to separate data into small batches to train the model.\n",
        "    train_loader  = DataLoader(\n",
        "        train_dataset, batch_size=params['batch_size'], shuffle=True, \n",
        "        num_workers=params['num_workers']\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=params['batch_size'], shuffle=False,\n",
        "        num_workers=params['num_workers']\n",
        "    )\n",
        "    \n",
        "    # Loading model into GPU.\n",
        "    model = PetNet()\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Setting criterion to calculate loss, optimizer and scheduler.\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                                 lr=params['lr'],\n",
        "                                 weight_decay=params['weight_decay'],\n",
        "                                 amsgrad=False)\n",
        "    \n",
        "    # Use the scheduler functions that we defined at section 5.2 to update the learning rate in optimizer.\n",
        "    scheduler = get_scheduler(optimizer)\n",
        "    \n",
        "    # Early stopping functions to stop the training process if the model is not improving after each epoch.\n",
        "    early_stopping = EarlyStopping(patience=2, verbose=True)\n",
        "    \n",
        "    # Training and validation loop\n",
        "    best_rmse = np.inf\n",
        "    best_epoch = np.inf\n",
        "    best_model_name = None\n",
        "    \n",
        "    # Epoch = how many times to repeat the training loop.\n",
        "    for epoch in range(40):\n",
        "        train_fn(train_loader, model, criterion, optimizer, epoch, params, scheduler)\n",
        "        predictions, valid_targets = validate_fn(val_loader, model, criterion, epoch, params)\n",
        "        rmse = round(mean_squared_error(valid_targets, predictions, squared=False), 3)\n",
        "        \n",
        "        # Condition loop to save the model with best score.\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            best_epoch = epoch\n",
        "            if best_model_name is not None:\n",
        "                os.remove(best_model_name)\n",
        "                \n",
        "            # Saving state_dict of the best model to rerun it later for inference.\n",
        "            torch.save(model.state_dict(),\n",
        "                       f\"{params['model']}_epoch_f{fold}.pth\")\n",
        "            best_model_name = f\"{params['model']}_epoch_f{fold}.pth\"\n",
        "        \n",
        "        # Evaluate the output rmse of the model to decide whether to stop the loop or not.\n",
        "        early_stopping(rmse, model)\n",
        "        \n",
        "        # Stop the training loop if the score doesn't improve after each epoch.\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "            \n",
        "    # Print summary\n",
        "    print('')\n",
        "    print(f'The best RMSE: {best_rmse} for fold {fold+1} was achieved on epoch: {best_epoch}')\n",
        "    print(f'The best saved model is: {best_model_name}')\n",
        "    best_models_of_each_fold.append(best_model_name)\n",
        "    rmse_tracker.append(best_rmse)\n",
        "    print(''.join(['#']*50))\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "print('')\n",
        "print(f'Average RMSE of all folds: {round(np.mean(rmse_tracker), 4)}')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKXhySbDy8Xc",
        "outputId": "f99a27dd-a074-464b-cae0-643ba989544e"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 00. Train. Loss: 0.657|RMSE: 18.045: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2230/2230 [34:02<00:00,  1.09it/s]\n",
            "Epoch: 00. Valid. Loss: 0.646|RMSE: 16.798: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [01:20<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss decreased (inf --> 18.492000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 01. Train. Loss: 0.650|RMSE: 17.155: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2230/2230 [34:04<00:00,  1.09it/s]\n",
            "Epoch: 01. Valid. Loss: 0.644|RMSE: 16.449: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [01:20<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss decreased (18.492000 --> 18.174000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 02. Train. Loss: 0.647|RMSE: 16.826: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2230/2230 [34:05<00:00,  1.09it/s]\n",
            "Epoch: 02. Valid. Loss: 0.643|RMSE: 16.262: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [01:19<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss decreased (18.174000 --> 18.066000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 03. Train. Loss: 0.644|RMSE: 16.343: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2230/2230 [34:02<00:00,  1.09it/s]\n",
            "Epoch: 03. Valid. Loss: 0.644|RMSE: 16.380: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [01:20<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 1 out of 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 04. Train. Loss: 0.641|RMSE: 15.986: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2230/2230 [34:01<00:00,  1.09it/s]\n",
            "Epoch: 04. Valid. Loss: 0.641|RMSE: 16.044: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [01:20<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss decreased (18.066000 --> 17.771000).  Saving model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 05. Train. Loss: 0.640|RMSE: 15.917: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2230/2230 [34:04<00:00,  1.09it/s]\n",
            "Epoch: 05. Valid. Loss: 0.643|RMSE: 16.351: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [01:20<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 1 out of 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 06. Train. Loss: 0.637|RMSE: 15.644: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2230/2230 [34:04<00:00,  1.09it/s]\n",
            "Epoch: 06. Valid. Loss: 0.642|RMSE: 16.207: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248/248 [01:20<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 2 out of 2\n",
            "Early stopping\n",
            "\n",
            "The best RMSE: 17.771 for fold 1 was achieved on epoch: 4\n",
            "The best saved model is: swin_large_patch4_window12_384_epoch_f0.pth\n",
            "##################################################\n",
            "\n",
            "Average RMSE of all folds: 17.771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5UsYa6NA11N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8mmqE5AZu9q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vq5dqpGmy-qz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}